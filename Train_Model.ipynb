{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tMYBPBfFQppu","outputId":"4203259e-971f-489b-ca5d-4729457fdafa"},"outputs":[{"name":"stdout","output_type":"stream","text":["Using device: cuda\n"]}],"source":["import os\n","import math\n","from IPython.display import clear_output\n","from collections import defaultdict\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import pandas as pd\n","from pandas.core.dtypes.cast import maybe_box_datetimelike\n","from copy import deepcopy\n","from tqdm import tqdm\n","\n","import torch\n","import torch.nn as nn\n","from torch.nn.modules.container import Sequential\n","from torch.utils.data import random_split, TensorDataset, Dataset, DataLoader\n","\n","import torchvision\n","from torchvision import datasets\n","\n","\n","from Model_and_trainer import CustomStructureDataset, First_CNN, ProgressPlotter\n","\n","import random\n","\n","\n","def set_random_seed(seed):\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed(seed)\n","    np.random.seed(seed)\n","    random.seed(seed)\n","\n","\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","print('Using device:', device)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0RI78uZK1DB1"},"outputs":[],"source":["#Loading train, val, test datasets\n","train_dataset = CustomStructureDataset('Data/train_data_files.csv', str_dir = 'Data/Dataset')\n","val_dataset = CustomStructureDataset('Data/Val_data.csv', str_dir = 'Data/Dataset_val')\n","test_dataset = CustomStructureDataset('Data/test_1.csv', str_dir = 'Data/Dataset_t1')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"15u4OIhtfq2e","outputId":"3f68e866-28fa-4ad3-ff44-ee69d2224e1c"},"outputs":[{"name":"stdout","output_type":"stream","text":["7.316920280456543 2.1500697135925293\n"]}],"source":["# Calculate mean and variance to normalize target values\n","train_labels = torch.Tensor(list(train_dataset.str_labels[2].astype(float)))\n","mean = torch.mean(train_labels).item()\n","std = torch.std(train_labels).item()\n","print(mean, std)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-bWnvGvmkPHe"},"outputs":[],"source":["\n","train_dataset.normalize = True # normalization of target values\n","train_dataset.mean = mean\n","train_dataset.std = std\n","train_dataset.train = True # if train = True, with probability 0.5, -1 is replaced by 1 and vice versa\n","train_dataset.transform = True # if transform = True, with probability 0.5 a mirror reflection occurs along one of the axes\n","\n","val_dataset.train = False\n","val_dataset.normalize = True\n","val_dataset.mean = mean\n","val_dataset.std = std\n","val_dataset.transform = None\n","\n","test_dataset.train = False\n","test_dataset.normalize = True\n","test_dataset.mean = mean\n","test_dataset.std = std\n","test_dataset.transform = None\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KFF1zu9cTtMy"},"outputs":[],"source":["from torch.utils.data import WeightedRandomSampler\n","batch_size = 32\n","\n","train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n","val_loader = DataLoader(val_dataset, shuffle=False, batch_size=1, num_workers=2)\n","test_loader = DataLoader(test_dataset, shuffle=False, batch_size=1, num_workers=2)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ukcolnDIDbZ7"},"outputs":[],"source":["import torch.nn as nn\n","import torch\n","from torch.nn.modules.container import Sequential\n","\n","class First_CNN(nn.Module):\n","\n","    def __init__(self):\n","        super().__init__()\n","        self.conv_stack = nn.Sequential(\n","            nn.Conv3d(10, 32, 7, padding=3, bias=False), # in channel=10, out=32\n","            nn.MaxPool3d(2), # size [32, length//2, width//2, high//2]\n","            nn.ReLU(),\n","            nn.BatchNorm3d(32),\n","\n","            nn.Conv3d(32, 64, 5, padding=2, bias=False), # in channel=32, out=64\n","            nn.MaxPool3d(2), # size [64,length//2//2,width//2//2, high//2//2]\n","            nn.ReLU(),\n","            nn.BatchNorm3d(64),\n","\n","            nn.Conv3d(64, 128, 3, padding=1, bias=False), # in channel=64, out=128\n","            nn.MaxPool3d(2), # size [128,length//2//2//2, width//2//2//2, high//2//2//2]\n","            nn.ReLU(),\n","            nn.BatchNorm3d(128),\n","\n","            nn.Conv3d(128, 256, 3, padding=1, bias=False), # in channel=128, out=256\n","            nn.MaxPool3d(2), # size [256,length//2//2//2//2, width//2//2//2//2, high//2//2//2//2]\n","            nn.ReLU(),\n","            nn.BatchNorm3d(256),\n","\n","            nn.Flatten(),\n","            nn.Linear(256*(81//2//2//2//2)*(81//2//2//2//2)*(41//2//2//2//2), 1000),\n","            nn.Dropout(0.3),\n","            nn.ReLU(),\n","            nn.Linear(1000, 200),\n","            nn.ReLU(),\n","            nn.Linear(200, 1))\n","\n","    def forward(self, x):\n","      x = self.conv_stack(x)\n","      return x\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nsBpb1HQV4oj"},"outputs":[],"source":["from torchmetrics import PearsonCorrCoef, R2Score, MeanSquaredError\n","\n","def train_func(model, criterion, optimizer, num_epochs, best_metrics):\n","  pp = ProgressPlotter(title=\"baseline\", groups=[\"loss\"])\n","  loss_hist = [] # for plotting\n","  acc_val_list = []\n","  val_loss_hist = [] # for plotting\n","  for epoch in range(num_epochs):\n","      model.train()\n","      hist_loss = 0\n","      for _, batch in tqdm(enumerate(train_loader, 0)): # get batch\n","          # parse batch\n","          structure, labels = batch\n","          structure, labels = structure.to(device=device, dtype=torch.float), labels.to(device=device, dtype=torch.float)\n","          # sets the gradients of all optimized tensors to zero.\n","          optimizer.zero_grad()\n","          # get outputs\n","          y_pred = model(structure)\n","          y_pred = torch.reshape(y_pred, (-1,))\n","          # calculate loss\n","          loss = criterion(y_pred, labels)\n","          # calculate gradients\n","          loss.backward()\n","          # performs a single optimization step (parameter update)\n","          optimizer.step()\n","          hist_loss += loss.item()\n","      loss_hist.append(hist_loss / len(train_loader))\n","\n","      pred_list_val = []\n","      labels_list_val = []\n","      pred_list_norm = []\n","      labels_list_norm = []\n","      pearson = PearsonCorrCoef().to(device)\n","      model.eval()\n","      with torch.no_grad():\n","        for struct_val, labels_val in val_loader:\n","          struct_val, labels_val = struct_val.to(device=device, dtype=torch.float), labels_val.to(device=device, dtype=torch.float)\n","          pred_val = model(struct_val.to(device))\n","          labels_list_val.append(labels_val[0].item()*std+mean)\n","          pred_list_val.append(torch.reshape(pred_val, (-1,))[0].item()*std+mean)\n","          pred_list_norm.append(torch.reshape(pred_val, (-1,))[0].item())\n","          labels_list_norm.append(labels_val[0].item())\n","      val_loss_hist = criterion(torch.Tensor(pred_list_norm).to(device), torch.Tensor(labels_list_norm).to(device))\n","      corr_coef= pearson(torch.Tensor(pred_list_val).to(device), torch.Tensor(labels_list_val).to(device))\n","      if corr_coef > best_metrics:\n","        best_metrics = corr_coef\n","        best_model = deepcopy(model)\n","\n","      y_pred_train = torch.tensor([])\n","      y_true_train = torch.tensor([])\n","      pearson = PearsonCorrCoef() # class for drawing training over epochs\n","      for i, data in tqdm(enumerate(train_loader)):\n","        struct_train, labels_train = data\n","        struct_train, labels_train = struct_train.to(device=device, dtype=torch.float), labels_train.to(device=device, dtype=torch.float)\n","        pred_train = model(struct_train.to(device)).cpu().detach()\n","        y_true_train = torch.cat((y_true_train, (labels_train*std+mean).cpu().detach()))\n","        y_pred_train = torch.cat((y_pred_train, pred_train*std+mean))\n","      corr_coef_train= pearson(torch.squeeze(y_pred_train), y_true_train)\n","\n","      print(f\"Epoch={epoch} loss={loss_hist[epoch]:.4f} val_loss: {round(val_loss_hist.item(),3)} val_corr: {round(corr_coef.item(),3)}\")\n","      pp.add_scalar(group=\"loss\", value=loss_hist[epoch], tag=\"train\")\n","      pp.add_scalar(group=\"loss\", value=val_loss_hist.item(), tag=\"val\")\n","      pp.add_scalar(\n","      group= 'corr', value=corr_coef_train.item(), tag=\"train\")\n","      pp.add_scalar(\n","      group= 'corr', value=corr_coef.item(), tag=\"val\")\n","      pp.display()\n","  return best_model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DlYJV-6iXltv"},"outputs":[],"source":["set_random_seed(42)\n","model = First_CNN()\n","model.to(device)\n","\n","criterion = nn.MSELoss()\n","\n","optimizer = torch.optim.AdamW(model.parameters(), lr=0.0001, weight_decay=1e-3)\n","\n","num_epochs = 20\n","best_model = train_func(model, criterion, optimizer, num_epochs, 0)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zrMMYjy-FNWA","scrolled":true},"outputs":[],"source":["set_random_seed(42)\n","best_model.train()\n","optimizer = torch.optim.AdamW(best_model.parameters(), lr=0.00001, weight_decay=1e-5)\n","num_epochs = 10\n","best_model2 = train_func(best_model, criterion, optimizer, num_epochs, 0.5)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pVVHCnojZpi0"},"outputs":[],"source":["\n","def validate(model, test_loader, device = \"cpu\"):\n","  model.eval()\n","  pred_list = []\n","  labels_list = []\n","  for struct, labels in test_loader:\n","      struct, labels = struct.to(device=device, dtype=torch.float), labels.to(device=device, dtype=torch.float)\n","      pred = model(struct.to(device))\n","      labels_list.append(labels[0].item()*std+mean)\n","      pred_list.append(round(torch.reshape(pred, (-1,))[0].item()*std+mean, 2))\n","\n","  pearson = PearsonCorrCoef().to(device)\n","  corr_coef= pearson(torch.Tensor(pred_list).to(device), torch.Tensor(labels_list).to(device))\n","\n","  mse = MeanSquaredError().to(device)\n","  rmse = math.sqrt(mse(torch.Tensor(pred_list).to(device), torch.Tensor(labels_list).to(device)).item())\n","\n","  print(f'PearsonCorr: {round(corr_coef.item(),2)}')\n","  print(f'RMSE: {round(rmse,3)}')\n","  return labels_list, pred_list"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CT3QJMplbsYv"},"outputs":[],"source":["a, b = validate(best_model2, test_loader, device=device)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"t8tu1xdzDbZ-"},"outputs":[],"source":["import scipy\n","scipy.stats.pearsonr(b, a)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"f08E6qf7d4Hr"},"outputs":[],"source":["import seaborn as sns\n","corr_df = pd.DataFrame(columns=['Pred_pKD', 'True_pKD'])\n","corr_df['Pred_pKD'] = b\n","corr_df['True_pKD'] = a\n","pd.set_option('display.max_rows', None)\n","pd.set_option('display.max_columns', None)\n","pd.set_option('display.max_colwidth', None)\n","\n","sns.set_context(rc={'figure.dpi': 500, 'font.size': 12})\n","fig = sns.jointplot(data=corr_df, x='True_pKD', y='Pred_pKD', palette='Set2', ylim=(2, 14), xlim=(2, 14))\n","\n","fig.figure.savefig(\"Result Model/model_test1.png\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MT4VRDbnDbZ-"},"outputs":[],"source":["corr_df.to_csv('Result Model/best_model_test_1.csv')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RaMqtcHnDbZ_"},"outputs":[],"source":["# 0: Hydrogen bond acceptors pr1 + Hydrogen bond donors pr2\n","# 1: Hydrogen bond donors pr1 + Hydrogen bond acceptors pr2\n","# 2: Hydrogen bond acceptors pr1 + Weak hydrogen bond donors pr2\n","# 3: Weak hydrogen bond donors pr1 + Hydrogen bond acceptors pr2\n","# 4: Positive charge atoms pr1 + Negative charge atoms pr2\n","# 5: Negative charge atoms pr1 + Positive charge atoms pr2\n","# 6: Hydrophobic atoms pr1 + Hydrophobic atoms pr2\n","# 7: Carbonyl carbons pr1 + Carbonyl carbons pr2\n","# 8: Carbonyl oxygens pr1 + Carbonyl oxygens pr2\n","# 9: Aromatic atoms pr1 + Aromatic atoms pr2\n","\n","channels_names = ['HB_Ac1+Don2', 'HB_Ac2+Don1', 'HB_Ac1+Weak_Don2', 'HB_Ac2+Weak_Don1', 'Pos1+Neg2', 'Pos2+Neg1', 'Hph1+Hph2', 'Carboxy_C1+Carboxy_C2', 'Carboxy_O1+Carboxy_O2', 'Arom1+Arom2']"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_xQ-QhGTDbZ_"},"outputs":[],"source":["# output of weights for each input channel\n","w0 = best_model2.conv_stack[0].weight\n","w0 = pd.DataFrame(np.transpose(w0.cpu().detach().numpy(), [0, 4, 2, 3, 1]).reshape((-1, 10)),\n","                  columns=channels_names)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RegkhynLDbZ_"},"outputs":[],"source":["# calculating the average value for significant neurons\n","diff = (w0.abs() > 0.001).mean()\n","diff.sort_values(ascending=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ViRYOEPcDbZ_"},"outputs":[],"source":["# range between 25th and 75th percentiles\n","perc_diff = ((w0.apply(lambda x: np.percentile(x, 75))\n","             - w0.apply(lambda x: np.percentile(x, 25)))\n","             .sort_values(ascending=False))\n","\n","# plotting spreads of weights for each of 10 channels\n","fig, ax = plt.subplots(figsize=(7, 6), dpi=300)\n","\n","sns.boxplot(data=w0, fliersize=0, orient='h', ax=ax)\n","ax.set_xlim(-0.055, 0.055)\n","ax.set_xticks(np.arange(-0.04, 0.05, 0.02))\n","ax.set_ylim(10, -1)\n","\n","fig.tight_layout()\n","fig.figure.savefig(\"Result Model/model_feat_imp.png\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TxHKUW0bDbZ_"},"outputs":[],"source":["test_dataset2 = CustomStructureDataset('Data/test_2.csv', str_dir = 'Data/Dataset_t2')\n","test_dataset2.train = False\n","test_dataset2.normalize = True\n","test_dataset2.mean = mean\n","test_dataset2.std = std\n","test_dataset2.transform = None\n","\n","test_loader2 = DataLoader(test_dataset2, shuffle=False, batch_size=1, num_workers=2)\n","\n","a, b = validate(best_model2, test_loader2, device=device)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zuDyeLylDbaA"},"outputs":[],"source":["scipy.stats.pearsonr(b, a)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RqDHN1grDbaA"},"outputs":[],"source":["corr_df = pd.DataFrame(columns=['Pred_pKD', 'True_pKD'])\n","corr_df['Pred_pKD'] = b\n","corr_df['True_pKD'] = a\n","pd.set_option('display.max_rows', None)\n","pd.set_option('display.max_columns', None)\n","pd.set_option('display.max_colwidth', None)\n","\n","sns.set_context(rc={'figure.dpi': 500, 'font.size': 12})\n","fig = sns.jointplot(data=corr_df, x='True_pKD', y='Pred_pKD', palette='Set2', ylim=(2, 14), xlim=(2, 14))\n","\n","fig.figure.savefig(\"Result Model/model_test_2.png\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qm_IxXF9DbaA"},"outputs":[],"source":["corr_df.to_csv('Result Model/best_model_test_2.csv')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bpVSiS77dJju"},"outputs":[],"source":["#torch.save(best_model2, 'Models/best_model.pt')"]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3.9 (Tensorflow 2.5)","language":"python","name":"tf"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.17"}},"nbformat":4,"nbformat_minor":0}